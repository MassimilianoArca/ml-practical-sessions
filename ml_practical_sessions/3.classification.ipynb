{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this notebook we are going to see the 2 approaches that are possible for classification:\n",
    "- **Discriminant function approach**:\n",
    "  + model a *function* that maps inputs to classes\n",
    "  + fit model to data\n",
    "- **Probabilistic discriminative approach**:\n",
    "  + model a *conditional probability* $P(C_k | x)$\n",
    "  + fit model to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the same dataset used in the second notebook and created in the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = pd.read_excel(os.path.join(data_path, 'clean_data.xlsx'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since no class is given, we are going to define custom ones on the target variable (DOC (mg/l))\n",
    "# using a custom value as threshold\n",
    "threshold = 8.0\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(station_df['DOC (mg/l)'], bins=100, alpha=0.5, color='b', edgecolor='black')\n",
    "\n",
    "plt.axvline(threshold, color='r', linestyle='dashed', linewidth=1, label=f\"Threshold ({threshold:.2f})\")\n",
    "\n",
    "plt.xlabel('DOC (mg/l)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.title('DOC (mg/l) distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a new column with the class labels\n",
    "station_df['DOC_class'] = np.where(station_df['DOC (mg/l)'] > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = station_df.drop(columns=['DOC (mg/l)', 'DOC_class']).copy()\n",
    "y = station_df['DOC_class'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant Function Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "At first, let us perform a classification with a perceptron classifier:\n",
    "- Hypothesis space: $y(\\mathbf{x}_n) = sgn(\\mathbf{w}^T \\mathbf{x}_n) = sgn(w_0 + x_{n1} w_1 + x_{n2} w_2)$;\n",
    "- Loss measure: Distance of misclassified points from the separating surface $L_P(\\mathbf{w}) = -\\sum_{n \\in \\mathcal{M}} \\mathbf{w}^T \\mathbf{x}_n C_n$;\n",
    "- Optimization method: Online Gradient Descent;\n",
    "\n",
    "where $sgn(\\cdot)$ is the sign function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perc_classifier = Perceptron(shuffle=False)\n",
    "perc_classifier.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = perc_classifier.predict(X_scaled)\n",
    "\n",
    "station_df['DOC_class_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to plot the DOC (mg/l) values and the predicted classes\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    station_df[station_df['DOC_class_pred'] == 0].index,\n",
    "    station_df[station_df['DOC_class_pred'] == 0]['DOC (mg/l)'],\n",
    "    color='b',\n",
    "    label=f\"Class < {threshold:.2f}\",\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    station_df[station_df['DOC_class_pred'] == 1].index,\n",
    "    station_df[station_df['DOC_class_pred'] == 1]['DOC (mg/l)'],\n",
    "    color='g',\n",
    "    label=f\"Class > {threshold:.2f}\",\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.axhline(threshold, color='r', linestyle='dashed', linewidth=1)\n",
    "\n",
    "plt.xlabel('DOC (mg/l)')\n",
    "plt.ylabel('Class')\n",
    "\n",
    "plt.title('DOC (mg/l) vs Predicted class')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performances of the chosen method, we need to compute the *confusion matrix* which tells us the number of points which have been correctly classified and those which have been misclassified.\n",
    "\n",
    " <table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>\n",
    "    <center> <img src='https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg' width=300 /> </center>\n",
    "    </th>\n",
    "    <th>\n",
    "</th>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Based on this matrix we can evaluate:\n",
    "- Accuracy: $Acc = \\frac{tp + tn}{N}$ fraction of the samples correctly classified in the dataset;\n",
    "- Precision $Pre = \\frac{tp}{tp + fp}$ fraction of samples correctly classified in the positive class among the ones classified in the positive class;\n",
    "- Recall: $Rec = \\frac{tp}{tp + fn}$ fraction of samples correctly classified in the positive class among the ones belonging to the positive class;\n",
    "- F1 score: $F1 = \\frac{2 \\cdot Pre \\cdot Rec}{Pre + Rec}$ harmonic mean of the precision and recall;\n",
    "\n",
    "where $tn$ is the number of true negatives, $fp$ is the number of false positives, $fn$ are the false negatives and $tn$ are the true negatives.\n",
    "Equivalently, we can look at the meaning of Precision and Recall by looking at the figure above.\n",
    "\n",
    "Remember that:\n",
    "- The higher these figures of merits the better the algorithm is performing.\n",
    "- These performance measures are **not** symmetric, but depends on the class we selected as positive.\n",
    "- Depending on the **application** one might switch the classes to have measures which better evaluate the predictive power of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=[f\"Class < {threshold:.2f}\", f\"Class > {threshold:.2f}\"],\n",
    "    yticklabels=[f\"Class < {threshold:.2f}\", f\"Class > {threshold:.2f}\"]\n",
    ")\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "While Perceptron tries to find the decision boundary directly in the input space, SVM finds the optimal hyperplane that maximally separates different classes by mapping the data from the input space to a higher-dimensional space (through the [Kernel Trick](https://en.wikipedia.org/wiki/Kernel_method)), where a linear decision boundary can be found. It is able to capture non-linearities in the data.\n",
    "\n",
    "We are going to compare the linear and the RBF kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_kernel_svm = svm.SVC(kernel='linear') # no mapping to higher dimension\n",
    "rbf_kernel_svm = svm.SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_kernel_svm.fit(X_scaled, y)\n",
    "rbf_kernel_svm.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin = lin_kernel_svm.predict(X_scaled)\n",
    "y_pred_rbf = rbf_kernel_svm.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the results\n",
    "conf_matrix_lin = confusion_matrix(y, y_pred_lin)\n",
    "conf_matrix_rbf = confusion_matrix(y, y_pred_rbf)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.heatmap(\n",
    "    conf_matrix_lin,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=[f\"Class < {threshold:.2f}\", f\"Class > {threshold:.2f}\"],\n",
    "    yticklabels=[f\"Class < {threshold:.2f}\", f\"Class > {threshold:.2f}\"],\n",
    "    ax=ax[0]\n",
    ")\n",
    "\n",
    "ax[0].set_title('Linear kernel SVM')\n",
    "ax[0].set_xlabel('Predicted')\n",
    "\n",
    "sns.heatmap(\n",
    "    conf_matrix_rbf,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=[f\"Class < {threshold:.2f}\", f\"Class > {threshold:.2f}\"],\n",
    "    yticklabels=[f\"Class < {threshold:.2f}\", f\"Class > {threshold:.2f}\"],\n",
    "    ax=ax[1]\n",
    ")\n",
    "\n",
    "ax[1].set_title('RBF kernel SVM')\n",
    "ax[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear kernel SVM accuracy: {accuracy_score(y, y_pred_lin)}\")\n",
    "print(f\"RBF kernel SVM accuracy: {accuracy_score(y, y_pred_rbf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear kernel SVM precision: {precision_score(y, y_pred_lin)}\")\n",
    "print(f\"RBF kernel SVM precision: {precision_score(y, y_pred_rbf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear kernel SVM recall: {recall_score(y, y_pred_lin)}\")\n",
    "print(f\"RBF kernel SVM recall: {recall_score(y, y_pred_rbf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear kernel SVM F1 score: {f1_score(y, y_pred_lin)}\")\n",
    "print(f\"RBF kernel SVM F1 score: {f1_score(y, y_pred_rbf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Discriminative Approach: Logistic Regression\n",
    "\n",
    "Let us change the methods for the classification task and use a Logistic regression classifier with two classes:\n",
    "- Hypothesis space: $y_n = y(x_n) = \\sigma(w_0 + x_{n1} w_1 + x_{n2} w_2)$;\n",
    "- Loss measure: Loglikelihood $L(\\mathbf{w}) = -\\sum_{n=1}^N  [C_n \\ln y_n + (1 - C_n) \\ln (1 - y_n)]$;\n",
    "- Optimization method: Gradient Descent;\n",
    "\n",
    "where the sigmoid function is defined as $\\sigma(x) = \\frac{1}{1 + e^{-x}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_classifier = LogisticRegression(penalty=None) # default is penalty='l2', that is l2 regularization\n",
    "log_classifier.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_classifier.predict(X_scaled)\n",
    "\n",
    "station_df['DOC_class_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to plot the DOC (mg/l) values and the predicted classes\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    station_df[station_df['DOC_class_pred'] == 0].index,\n",
    "    station_df[station_df['DOC_class_pred'] == 0]['DOC (mg/l)'],\n",
    "    color='b',\n",
    "    label=f\"Class < {threshold:.2f}\",\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    station_df[station_df['DOC_class_pred'] == 1].index,\n",
    "    station_df[station_df['DOC_class_pred'] == 1]['DOC (mg/l)'],\n",
    "    color='g',\n",
    "    label=f\"Class > {threshold:.2f}\",\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.axhline(threshold, color='r', linestyle='dashed', linewidth=1)\n",
    "\n",
    "plt.xlabel('DOC (mg/l)')\n",
    "plt.ylabel('Class')\n",
    "\n",
    "plt.title('DOC (mg/l) vs Predicted class')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=[f\"Class < {threshold:.2f}\", f\"Class > {threshold:.2f}\"],\n",
    "    yticklabels=[f\"Class < {threshold:.2f}\", f\"Class > {threshold:.2f}\"]\n",
    ")\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
