{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "In this exercise section, we will see some of the methods to select a model in the first place. These includes:\n",
    "\n",
    "- backward feature selection, which is a supervised methodology to identify the relevant features for a given task;\n",
    "- regularization with the $\\lambda$ parameter of Lasso;\n",
    "- Hyperparameter tuning of XGBoost and MLP Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the clean data from the previous lesson\n",
    "station_df = pd.read_excel(os.path.join(data_path, \"clean_data.xlsx\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Feature Selection\n",
    "\n",
    "Let us try to identify a convenient subset of $k$ features for our regression task. In principle, we would have to train $\\binom{9}{k}$ models, one for each $k \\in \\{ 1, ..., 9 \\}$, and to compare one model against the others. This sounds quite inefficient. Instead, we start from the complete set of features and we try to iteratively remove the least relevant feature. The relevance of a feature can be specified in different ways. For simple models like linear regression, usually the one with the coefficient with the highest p-value is removed. We stop when there are no features with a p-value > 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into features and target\n",
    "X = station_df.drop(columns=[\"DOC (mg/l)\"]).copy()\n",
    "y = station_df[\"DOC (mg/l)\"].copy()\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# revert to dataframe\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first train a linear regression model with all the features\n",
    "X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "\n",
    "initial_model = sm.OLS(y_train, X_train_scaled).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def backward_feature_selection(X_bfs, y_bfs):\n",
    "    features = list(X_bfs.columns)\n",
    "    best_features = features.copy()\n",
    "    best_model = None\n",
    "    best_aic = initial_model.aic\n",
    "\n",
    "    # while there are features to remove\n",
    "    while len(features) > 1:\n",
    "        \n",
    "        # Initialize the best feature and the current best features\n",
    "        current_best_features = best_features.copy()\n",
    "\n",
    "        # for each feature\n",
    "        for feature in features:\n",
    "            remaining_features = [f for f in features if f != feature]  # Remove one feature\n",
    "            X_step = X_bfs[remaining_features]\n",
    "            X_step = sm.add_constant(X_step)\n",
    "            model = sm.OLS(y_bfs, X_step).fit()\n",
    "            aic = model.aic\n",
    "\n",
    "            # If the AIC of the new model is better, store the new AIC and the new features\n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                # Store the new best features\n",
    "                best_features = remaining_features\n",
    "                best_model = model\n",
    "                removed_feature = feature  # Store the removed feature\n",
    "            \n",
    "        # If no feature was removed, stop the loop\n",
    "        if best_features == current_best_features:\n",
    "            print(\"No further improvement in AIC. Stopping.\")\n",
    "            break\n",
    "\n",
    "        print(f\"New best AIC: {best_aic} | Removed feature: {removed_feature}\")\n",
    "        # Update the features to the current best features for the next iteration\n",
    "        features = best_features.copy()\n",
    "\n",
    "    return best_features, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features, model = backward_feature_selection(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Selected features: {selected_features}\")\n",
    "print()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "We are going to use Lasso regularization, which is indeed a model selection and feature selection technique, as it adds a $L_1$-norm to the loss function shrinking the coefficients of less important features toward zero. We are going to perform cross validation for a set of values of the regularization term $\\lambda$. Note that if $\\lambda = 0$, we go back to classic OLS linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into features and target\n",
    "X = station_df.drop(columns=[\"DOC (mg/l)\"]).copy()\n",
    "y = station_df[\"DOC (mg/l)\"].copy()\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# revert to dataframe\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_valid_scaled = pd.DataFrame(X_valid_scaled, columns=X.columns, index=X_valid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show how the coefficients change with different lambdas (alphas in sklearn)\n",
    "# together with the model performance\n",
    "\n",
    "alphas = np.linspace(1e-5, 1, 1000)\n",
    "coeffs = []\n",
    "errors = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    coeffs.append(model.coef_)\n",
    "    errors.append(mean_squared_error(y_valid, model.predict(X_valid_scaled)))\n",
    "    \n",
    "coeffs = np.array(coeffs)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i in range(coeffs.shape[1]):\n",
    "    ax1.plot(alphas, coeffs[:, i], label=model.feature_names_in_[i])\n",
    "    \n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Alpha (log scale)\")\n",
    "ax1.set_ylabel(\"Coefficient Value\")\n",
    "ax1.set_title(\"Lasso Coefficients and Model Performance vs. Alpha\")\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "ax1.grid(True)\n",
    "    \n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(alphas, errors, color=\"red\", linestyle=\"dashed\", label=\"MSE\")\n",
    "ax2.set_ylabel(\"MSE\", color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as alpha increases, coefficients shrink to zero, leading to a too much simple model, where only the constant term is left, meaning that we get a constant mse. When alpha is small, many coefficients retain their magnitude. When alpha reaches 10-3, coefficients start to shrink and around alpha = 10-2 a subset of features has a coefficient close to zero, meaning that Lasso is performing feature selection. We can also see that we have a slight decrease of the MSE, meaning that Lasso feature selection is leading to a better representation of the relationship between the target and features variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal values for a modelâ€™s hyperparameters, which are parameters that are not learned directly from the data but rather set before training. These hyperparameters control how the model learns and generalizes.\n",
    "\n",
    "We are going to see two simple techniques, Grid Search (systematically evaluates predefined hyperparameter values) and Random Search (samples random combinations within a given range), but more sophisticated ones can also be used, such as [Bayesian Optimization](https://en.wikipedia.org/wiki/Bayesian_optimization#:~:text=Bayesian%20optimization%20is%20a%20sequential,expensive%2Dto%2Devaluate%20functions.) or frameworks like [Optuna](https://optuna.readthedocs.io/en/stable/).\n",
    "\n",
    "The models we are going to tune are MLP Neural Network and XGBoost. We use the scikit-learn package which offers both Grid Search and Random Search. For further information, [GridSearchCV](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) and [RandomSearchCV](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV).\n",
    "\n",
    "Hyperparameter tuning is usually performed with cross-validation in order to ensure that the selected hyperparameters generalize well on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into features and target\n",
    "X = station_df.drop(columns=[\"DOC (mg/l)\"]).copy()\n",
    "y = station_df[\"DOC (mg/l)\"].copy()\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# revert to dataframe\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron in the neural network is characterized by an activation function which is a transformation of its input. Different activation functions are present. The most used are usually:\n",
    "\n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Tanh\n",
    "- Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu activation function\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.maximum(0, x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y)\n",
    "plt.title(\"ReLu Activation Function\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh activation function\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.tanh(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Tanh Activation Function\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Sigmoid Activation Function\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to define the hyperparameters to search and their ranges of values\n",
    "# since there are a lof of hyperparameters, we just define a few of them\n",
    "\n",
    "mlp_params = {\n",
    "    \"hidden_layer_sizes\": [(20,), (50,) , (20, 20), (50, 50)], # number of neurons in each layer\n",
    "    \"activation\": [\"relu\", \"logistic\", \"tanh\" ], # activation function\n",
    "    \"solver\": [\"adam\", \"sgd\"], # optimization algorithm\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"], # learning rate schedule\n",
    "    \"batch_size\": [16, 32,], # size of minibatches\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MLPRegressor(max_iter=1000)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=mlp_params,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\", # we want to minimize the MSE\n",
    "    verbose=3, # change to higher values to see more information\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could take a couple of minutes\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# print the results of the best model found\n",
    "print(\"The best parameters are:\")\n",
    "pprint.pprint(grid_search.best_params_)\n",
    "print()\n",
    "print(f\"The best score is: {-grid_search.best_score_.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MLPRegressor(max_iter=1000)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    n_iter=10,\n",
    "    estimator=estimator,\n",
    "    param_distributions=mlp_params,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\", # we want to minimize the MSE\n",
    "    verbose=3, # change to higher values to see more information\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results of the best model found\n",
    "print(\"The best parameters are:\")\n",
    "pprint.pprint(random_search.best_params_)\n",
    "print()\n",
    "print(f\"The best score is: {-random_search.best_score_.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use tree-based models\n",
    "xgb_params = {\n",
    "    \"n_estimators\": [20, 50, 100], # number of trees\n",
    "    \"eta\": [0.01, 0.1, 0.3, 0.5, 0.7], # learning rate\n",
    "    \"max_depth\": [3, 5, 7], # maximum depth of the trees\\\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=xgb_params,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\", # we want to minimize the MSE\n",
    "    verbose=3, # change to higher values to see more information\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results of the best model found\n",
    "print(\"The best parameters are:\")\n",
    "pprint.pprint(grid_search.best_params_)\n",
    "print()\n",
    "print(f\"The best score is: {-grid_search.best_score_.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_distributions=xgb_params,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\", # we want to minimize the MSE\n",
    "    verbose=3, # change to higher values to see more information\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results of the best model found\n",
    "print(\"The best parameters are:\")\n",
    "pprint.pprint(random_search.best_params_)\n",
    "print()\n",
    "print(f\"The best score is: {-random_search.best_score_.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that grid search evaluates every possible combination of parameters' values given, while random search just a subset. The trade-off is that with grid search, the combination with the smallest error will be surely found but the computation time will be much higher with respect to random search as it is basically a brute-forcing technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
