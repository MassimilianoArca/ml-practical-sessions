{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis, Linear Regression and Model Comparison \n",
    "\n",
    "In this notebook we are going to consider historical measurements of water quality data coming from a surface water station in Berlin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import the necessary libraries to load and handle data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data path\n",
    "data_path = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use pandas to load the data into a **pandas.DataFrame**.\n",
    "Since we received three csv files, one for measurements regarding water quality parameters, one for flow rate measurements and one for rain precipitations, we are going to load three csv files in three separate pandas.DataFrame that we are going to merge later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sw_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        data_path, \"time-series_surface-water_quality.csv\"\n",
    "    )\n",
    ")\n",
    "flow_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        data_path, \"time-series_surface-water_flow.csv\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a first look at the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand them, it could be helpful to translate them to english :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sw_df.rename(\n",
    "    columns={\n",
    "        \"Messstelle\": \"Station\",\n",
    "        \"Messstellennummer\": \"Station ID\",\n",
    "        \"Datum\": \"DateTime\",\n",
    "        \"Einheit\": \"Unit\",\n",
    "        \"Wert\": \"Value\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "ts_sw_df.drop(\n",
    "    columns=[\n",
    "        \"Entnahmetiefe [m]\",\n",
    "        \"Messmethode\",\n",
    "        \"Vorzeichen\",\n",
    "        \"Bestimmungsgrenze\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the translation dictionary for the variables we are interested in\n",
    "variables_mapping = {\n",
    "    \"Lufttemperatur\": \"Air Temperature (°C)\",\n",
    "    \"Wassertemperatur\": \"Water Temperature (°C)\",\n",
    "    \"Leitfähigkeit\": \"Conductivity (µS/cm)\",\n",
    "    \"Ammonium-Stickstoff\": \"Ammonium (mg/l)\",\n",
    "    \"Nitrat-Stickstoff\": \"Nitrate (mg/l)\",\n",
    "    \"pH-Wert\": \"pH\",\n",
    "    \"DOC (Gelöster organischer Kohlenstoff)\": \"DOC (mg/l)\",\n",
    "    \"Sauerstoff-Gehalt\": \"Dissolved Oxygen (mg/l)\",\n",
    "}\n",
    "# Filter the data frame to only include the variables we are interested in\n",
    "ts_sw_df = ts_sw_df[ts_sw_df[\"Parameter\"].isin(variables_mapping.keys())]\n",
    "\n",
    "# Rename the variables\n",
    "ts_sw_df[\"Parameter\"] = ts_sw_df[\"Parameter\"].map(variables_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df.rename(\n",
    "    columns={\n",
    "        \"Messstellennummer\": \"Station ID\",\n",
    "        \"Datum\": \"DateTime\",\n",
    "        \"Tagesmittelwert\": \"Flow Rate (m³/s)\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "flow_df.drop(\n",
    "    columns=[\n",
    "        \"Parameter\",\n",
    "        \"Einheit\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we are going to work just on the data coming from station 325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sw_df = ts_sw_df[ts_sw_df[\"Station ID\"] == 305]\n",
    "\n",
    "# close to the station 325\n",
    "flow_df = flow_df[flow_df[\"Station ID\"] == 5803200]\n",
    "\n",
    "# Remove the Station ID column as it is not needed anymore\n",
    "flow_df.drop(columns=[\"Station ID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the DataFrame\n",
    "ts_sw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to have a column for each variable, we are going to pivot the DataFrame\n",
    "ts_sw_df = ts_sw_df.pivot(\n",
    "    index=\"DateTime\", columns=\"Parameter\", values=\"Value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to merge the water quality data with the flow data, using the DateTime as the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to convert the DateTime to a datetime object\n",
    "ts_sw_df.index = pd.to_datetime(ts_sw_df.index)\n",
    "\n",
    "flow_df[\"DateTime\"] = pd.to_datetime(flow_df[\"DateTime\"])\n",
    "flow_df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "# We are going to merge the two DataFrames on just the date (not the time)\n",
    "ts_sw_df.index = ts_sw_df.index.date\n",
    "flow_df.index = flow_df.index.date\n",
    "\n",
    "# Merge the two DataFrames with a left join since\n",
    "# we want to keep all the water quality data even if there is no flow data\n",
    "station_df = ts_sw_df.merge(\n",
    "    flow_df, left_index=True, right_index=True, how='left'\n",
    ")\n",
    "\n",
    "# Recast the index to a datetime object\n",
    "station_df.index = pd.to_datetime(station_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the DataFrame\n",
    "station_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather some information regarding the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_station_df = pd.DataFrame(\n",
    "    index=pd.Index(\n",
    "        [\n",
    "            \"N Samples\",\n",
    "            \"% Missing Values\",\n",
    "            \"Frequency (days)\",\n",
    "            \"Mean\",\n",
    "            \"Std\",\n",
    "            \"Start Date\",\n",
    "            \"End Date\",\n",
    "        ],\n",
    "        name=\"Info\",\n",
    "    ),\n",
    "    columns=station_df.columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the information in the station_info_df\n",
    "for column in station_df.columns:\n",
    "    \n",
    "    # Get the column of interest\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    # Get the start and end date of the time series,\n",
    "    # from the first non missing value to the last one\n",
    "    start_date = df.dropna().index.min()\n",
    "    end_date = df.dropna().index.max()\n",
    "\n",
    "    # Filter the DataFrame to only include the period of interest\n",
    "    df = df[start_date:end_date]\n",
    "\n",
    "    # Calculate the percentage of missing values in the period of interest\n",
    "    missing_values = df.isna().sum() / df.shape[0] * 100\n",
    "\n",
    "    # Store the number of valid samples\n",
    "    info_station_df.loc[\"N Samples\", column] = (\n",
    "        station_df[column].dropna().shape[0]\n",
    "    )\n",
    "    # Store the percentage of missing values\n",
    "    info_station_df.loc[\n",
    "        \"% Missing Values\", column\n",
    "    ] = missing_values.round(3)\n",
    "    \n",
    "    # Store the most common frequency of the time series\n",
    "    info_station_df.loc[\"Frequency (days)\", column] = (\n",
    "        station_df.index.to_series().diff().value_counts().index[0].days\n",
    "    )\n",
    "    \n",
    "    # Store the mean and standard deviation of the time series\n",
    "    info_station_df.loc[\"Mean\", column] = df.mean().round(3)\n",
    "    info_station_df.loc[\"Std\", column] = df.std().round(3)\n",
    "    \n",
    "    # Store the start and end date of the time series\n",
    "    info_station_df.loc[\"Start Date\", column] = start_date.strftime(\"%Y-%m-%d\")\n",
    "    info_station_df.loc[\"End Date\", column] = end_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water Quality + Flow Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary table we can see that the % of missing values is quite small for each parameter, and the most common sampling frequency is monthly. Since we need to have a common time interval for every variable, we are going to cut each time series from the latest Start Date (Flow Rate) to the earliest End Date (Ammonium)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = station_df[\"Flow Rate (m³/s)\"].dropna().index.min()\n",
    "end_date = station_df[\"Ammonium (mg/l)\"].dropna().index.max()\n",
    "\n",
    "station_df = station_df[start_date:end_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start to plot the variables. We are going to use matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in station_df.columns:\n",
    "    \n",
    "    df = station_df[column].copy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot the time series\n",
    "    plt.plot(station_df.index, station_df[column], label=column)\n",
    "    \n",
    "    # Add a vertical line for each date with missing values\n",
    "    missing_dates = station_df[station_df[column].isna()].index\n",
    "    for date in missing_dates:\n",
    "        plt.axvline(date, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "        \n",
    "    # add legend for missing values line if there are any\n",
    "    if len(missing_dates) > 0:\n",
    "        plt.plot([], color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Missing Values\")\n",
    "        \n",
    "    # Add a title\n",
    "    plt.title(column)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.ylabel(column)\n",
    "    plt.xlabel(\"Date\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs we see that the data is quite clean. For the outliers' removal process we are going to be more conservative as water quality data has strong seasonal effects and could be noisy.\n",
    "\n",
    "We are going to use [Prophet](https://facebook.github.io/prophet/) to detect outliers. What we are going to do is:\n",
    "- compute the residuals of the Prophet predictions:\n",
    "$$\n",
    "r(t) = y(t) - \\hat{y}(t)\n",
    "$$\n",
    "\n",
    "- define an anomaly if:\n",
    "$$\n",
    "|r(t)| > f * (\\hat{y}_{hb}(t) - \\hat{y}_{lb}(t))\n",
    "$$\n",
    "\n",
    "where *f* is an anomaly factor that can be tuned based on your choice. An outlier is identified as such if the residual of its prediction is greater than *f* times the difference between the upper and lower prediction bounds provided by Prophet, indicating that the actual value significantly deviates from the expected range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from prophet import Prophet\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 3 # to be more conservative, usually 1.5 to 3\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "    # ===== Prophet =====\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    \n",
    "    # We don't want to predict anything, just get the forecast for the training data\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error (residual) and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    \n",
    "    # We are going to consider an anomaly if the error is greater than factor * uncertainty\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    anomaly = forecasting_final[forecasting_final[\"anomaly\"] == \"Yes\"]\n",
    "    \n",
    "    axs[0].plot(forecasting_final[\"ds\"], forecasting_final[\"y\"], label=\"Actual\")\n",
    "    axs[0].plot(forecasting_final[\"ds\"], forecasting_final[\"yhat\"], label=\"Prophet Prediction\")\n",
    "    \n",
    "    if not anomaly.empty:\n",
    "        axs[0].scatter(anomaly[\"ds\"], anomaly[\"y\"], label=\"Prophet Outliers\", color=\"red\")\n",
    "    \n",
    "    axs[1].plot(forecasting_final[\"ds\"], forecasting_final[\"error\"], label=\"Error\")\n",
    "    axs[1].plot(forecasting_final[\"ds\"], forecasting_final[\"uncertainty\"], label=\"Uncertainty\")\n",
    "    \n",
    "    if not anomaly.empty:\n",
    "        axs[1].scatter(anomaly[\"ds\"], anomaly[\"error\"], label=\"Prophet Outliers\", color=\"red\")\n",
    "\n",
    "    \n",
    "    axs[0].set_title(column)\n",
    "    axs[0].set_xlabel(\"Date\")\n",
    "    axs[0].set_ylabel(column)\n",
    "    axs[0].legend()\n",
    "    axs[0].grid()\n",
    "    \n",
    "    axs[1].set_title(\"Error and Uncertainty\")\n",
    "    axs[1].set_xlabel(\"Date\")\n",
    "    axs[1].set_ylabel(\"Error and Uncertainty\")\n",
    "    axs[1].legend()\n",
    "    axs[1].grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, just two points have been labeled as anomalies. We are going to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to be processed\n",
    "\n",
    "for column in station_df.columns:\n",
    "    df = station_df[column].copy()\n",
    "\n",
    "    df.index.name = \"ds\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df.rename(columns={column: \"y\"}, inplace=True)\n",
    "\n",
    "    # using prophet\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    \n",
    "    # Make predictions for both columns\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Merging forecasted data with the original data\n",
    "    forecasting_final = pd.merge(\n",
    "        forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "        df,\n",
    "        how=\"inner\",\n",
    "        on=\"ds\",\n",
    "    )\n",
    "\n",
    "    # Calculate the prediction error and uncertainty\n",
    "    forecasting_final[\"error\"] = (\n",
    "        forecasting_final[\"y\"] - forecasting_final[\"yhat\"]\n",
    "    )\n",
    "    forecasting_final[\"uncertainty\"] = (\n",
    "        forecasting_final[\"yhat_upper\"]\n",
    "        - forecasting_final[\"yhat_lower\"]\n",
    "    )\n",
    "\n",
    "    # Anomaly detection\n",
    "    forecasting_final[\"anomaly\"] = forecasting_final.apply(\n",
    "        lambda x: \"Yes\"\n",
    "        if (np.abs(x[\"error\"]) > factor * x[\"uncertainty\"])\n",
    "        else \"No\",\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # remove the outliers by replacing them with NaN\n",
    "    forecasting_final.loc[\n",
    "        forecasting_final[\"anomaly\"] == \"Yes\", \"y\"\n",
    "    ] = np.nan\n",
    "\n",
    "    df = forecasting_final[[\"ds\", \"y\"]]\n",
    "\n",
    "    df.set_index(\"ds\", inplace=True)\n",
    "\n",
    "    df = df.rename(columns={\"y\": column})\n",
    "\n",
    "    station_df.loc[df.index, column] = df[column]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the analysis easier, we are going to resample the data to a monthly frequency\n",
    "and calculate the mean value for each month if there are multiple samples in the same month.\n",
    "\n",
    "Also, the resampling will ease the missing values imputation as we will have a regular time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = station_df.resample(\"ME\").mean()\n",
    "\n",
    "# We are going to impute the missing values using the interpolation method for the time series\n",
    "# This method will fill the missing values by interpolating between the two closest non missing values\n",
    "station_df = station_df.interpolate(method=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the plots of preprocessed data, with missing values filled\n",
    "for column in station_df.columns:\n",
    "    \n",
    "    df = station_df[column].copy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot the time series\n",
    "    plt.plot(station_df.index, station_df[column], label=column)\n",
    "    \n",
    "    # Add a vertical line for each date with missing values\n",
    "    missing_dates = station_df[station_df[column].isna()].index\n",
    "    for date in missing_dates:\n",
    "        plt.axvline(date, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "        \n",
    "    # add legend for missing values line if there are any\n",
    "    if len(missing_dates) > 0:\n",
    "        plt.plot([], color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Missing Values\")\n",
    "        \n",
    "    # Add a title\n",
    "    plt.title(column)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.ylabel(column)\n",
    "    plt.xlabel(\"Date\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a final description of the data\n",
    "station_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteorological Data\n",
    "\n",
    "As a final step to build the final dataset, we need to add the precipitation measurements available from an airport quite distant from the station. In order to assess similarity between the two sampling points, we have to evaluate the similarity of the measurements of common variables. The only variable in common between the two DataFrames is the Air Temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "meteo_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        data_path, \"produkt_klima_tag_19480101_20231231_00433.csv\",\n",
    "    ),\n",
    "    sep=\";\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df.rename(\n",
    "    columns={\n",
    "        \"STATIONS_ID\": \"Station ID\",\n",
    "        \"MESS_DATUM\": \"DateTime\",\n",
    "        \"  FX\": \"Wind Speed Max (m/s)\",\n",
    "        \"  FM\": \"Wind Speed Mean (m/s)\",\n",
    "        \" RSK\": \"Cumulated Rainfall (mm)\",\n",
    "        \"RSKF\": \"Cumulated Rainfall Type\",\n",
    "        \" SDK\": \"Sunshine Duration (hours)\",\n",
    "        \"SHK_TAG\": \"Snow Height (cm)\",\n",
    "        \"  NM\": \"Cloud Coverage (1/8)\",\n",
    "        \" VPM\": \"Vapor Pressure (hPa)\",\n",
    "        \"  PM\": \"Pressure (hPa)\",\n",
    "        \" TMK\": \"Temperature Mean (°C)\",\n",
    "        \" UPM\": \"Humidity (%)\",\n",
    "        \" TXK\": \"Temperature Max at 2m (°C)\",\n",
    "        \" TNK\": \"Temperature Min at 2m (°C)\",\n",
    "        \" TGK\": \"Temperature Min at 5cm (°C)\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "meteo_df[\"DateTime\"] = pd.to_datetime(meteo_df[\"DateTime\"], format=\"%Y%m%d\")\n",
    "meteo_df.set_index(\"DateTime\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just take the necessary columns\n",
    "meteo_df = meteo_df[\n",
    "    [\n",
    "        \"Temperature Mean (°C)\",\n",
    "        \"Cumulated Rainfall (mm)\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the DataFrame\n",
    "info_meteo_df = pd.DataFrame(\n",
    "    index=pd.Index(\n",
    "        [\n",
    "            \"N Samples\",\n",
    "            \"% Missing Values\",\n",
    "            \"Frequency (days)\",\n",
    "            \"Mean\",\n",
    "            \"Std\",\n",
    "            \"Start Date\",\n",
    "            \"End Date\",\n",
    "        ],\n",
    "        name=\"Info\",\n",
    "    ),\n",
    "    columns=meteo_df.columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in meteo_df.columns:\n",
    "    start_date = meteo_df[column].dropna().index.min()\n",
    "    \n",
    "    end_date = meteo_df[column].dropna().index.max()\n",
    "\n",
    "    df = meteo_df[start_date:end_date][column]\n",
    "\n",
    "    print(f\"Start date for {column}: {start_date}\")\n",
    "    print(f\"End date for {column}: {end_date}\")\n",
    "\n",
    "    missing_values = df.isna().sum() / df.shape[0]\n",
    "    print(f\"Missing values for {column}: {missing_values}\")\n",
    "\n",
    "    frequency = df.index.to_series().diff().value_counts().index[0].days\n",
    "    print(f\"Frequency for {column}: {frequency}\")\n",
    "\n",
    "    info_meteo_df.loc[\"N Samples\", column] = (\n",
    "        meteo_df[column].dropna().shape[0]\n",
    "    )\n",
    "    info_meteo_df.loc[\"% Missing Values\", column] = missing_values\n",
    "    info_meteo_df.loc[\"Frequency (days)\", column] = frequency\n",
    "    \n",
    "    info_meteo_df.loc[\"Mean\", column] = df.mean()\n",
    "    info_meteo_df.loc[\"Std\", column] = df.std()\n",
    "    \n",
    "    info_meteo_df.loc[\"Start Date\", column] = start_date.strftime(\"%Y-%m-%d\")\n",
    "    info_meteo_df.loc[\"End Date\", column] = end_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_meteo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are not missing values in the time series. The time range is very long, from 1948 to 2023. The sampling frequency is daily, so we need to adjust it to monthly.\n",
    "\n",
    "However, the standard deviation of the Cumulated Rainfall is a bit high, let's visualize the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspect the data\n",
    "\n",
    "for column in meteo_df.columns:\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.plot(meteo_df.index, meteo_df[column], label=column)\n",
    "\n",
    "    plt.title(column)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulated rainfall has some errors in the measurements, let's delete the measurements that are < 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_df.loc[\n",
    "    meteo_df[\"Cumulated Rainfall (mm)\"] < 0, [\"Cumulated Rainfall (mm)\"]\n",
    "] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"Cumulated Rainfall (mm)\"\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(meteo_df.index, meteo_df[column], label=column)\n",
    "\n",
    "plt.title(column)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(column)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are now reasonable. Let's resample to monthly measurements and fill the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the data to monthly\n",
    "meteo_df = meteo_df.resample(\"ME\").mean()\n",
    "\n",
    "meteo_df = meteo_df.interpolate(method=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to compare the air temperature measurements of the two sites to understand if they can be considered similar, in order to merge the rainfall measurements in the final dataset. We compute the pearson correlation as a proxy of how much the two time series are correlated. We also show visual evidence as well as a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pearson correlation\n",
    "\n",
    "start_date = station_df.index.min()\n",
    "end_date = station_df.index.max()\n",
    "\n",
    "# take the common date range with the airport\n",
    "start_date = max(start_date, meteo_df.index.min())\n",
    "end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "# take the common date range with the station\n",
    "df = station_df[\n",
    "    (station_df.index >= start_date)\n",
    "    & (station_df.index <= end_date)\n",
    "].copy()\n",
    "\n",
    "# compute pearson correlation\n",
    "corr, _ = pearsonr(\n",
    "    airport_df[\"Temperature Mean (°C)\"],\n",
    "    df[\"Air Temperature (°C)\"],\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot Airport data\n",
    "ax.plot(airport_df.index, airport_df[\"Temperature Mean (°C)\"], color=\"blue\", label=\"Airport\")\n",
    "\n",
    "# Plot Station data\n",
    "ax.plot(df.index, df[\"Air Temperature (°C)\"], color='green', label=f\"Surface Station\")\n",
    "\n",
    "# Add the correlation to the plot\n",
    "ax.text(0.01, 0.95, f\"Pearson Correlation: {corr:.2f}\", transform=ax.transAxes, fontsize=18, verticalalignment='top')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Air Temperature (°C)\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Air Temperature\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only variable in common between the two DataFrames is the Temperature Mean\n",
    "start_date = station_df.index.min()\n",
    "end_date = station_df.index.max()\n",
    "\n",
    "# take the common date range with the airport\n",
    "start_date = max(start_date, meteo_df.index.min())\n",
    "end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "# take the common date range with the station\n",
    "df = station_df[\n",
    "    (station_df.index >= start_date)\n",
    "    & (station_df.index <= end_date)\n",
    "].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot Data\n",
    "ax.scatter(airport_df[\"Temperature Mean (°C)\"], df[\"Air Temperature (°C)\"], \n",
    "           s=64, color=\"blue\", alpha=0.7, label=\"Data\")\n",
    "\n",
    "# Add line on bisector\n",
    "ax.plot([-10, 40], [-10, 40], color=\"red\", linewidth=2, linestyle=\"--\", label=\"Bisector\")\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel(\"Airport\")\n",
    "ax.set_title(\"Air Temperature\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have strong correlation between the two measurements, and visually we can see that the seasonal pattern is very similar. We can establish that they are similar, and we can merge the rainfall into the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df['Cumulated Rainfall (mm)'] = np.nan\n",
    "\n",
    "start_date = station_df.index.min()\n",
    "end_date = station_df.index.max()\n",
    "\n",
    "# take the common date range with the airport\n",
    "start_date = max(start_date, meteo_df.index.min())\n",
    "end_date = min(end_date, meteo_df.index.max())\n",
    "\n",
    "airport_df = meteo_df[start_date:end_date].copy()\n",
    "\n",
    "# take the common date range with the station\n",
    "# Identify the indices in sw_df that match the station_id and are within the date range\n",
    "station_df.loc[\n",
    "    (station_df.index >= start_date)\n",
    "    & (station_df.index <= end_date),\n",
    "    \"Cumulated Rainfall (mm)\",\n",
    "] = airport_df[\"Cumulated Rainfall (mm)\"]\n",
    "\n",
    "# interpolate the missing value of the Cumulated Rainfall\n",
    "station_df[\"Cumulated Rainfall (mm)\"] = station_df[\"Cumulated Rainfall (mm)\"].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot Data\n",
    "ax.plot(station_df.index, station_df[\"Cumulated Rainfall (mm)\"], color=\"blue\", label=\"Airport\")\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Cumulated Rainfall (mm)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final overview of the data\n",
    "station_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the final DataFrame for future notebooks\n",
    "station_df.to_excel(\n",
    "    os.path.join(data_path, \"clean_data.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "We can now start performing linear regression on historical data to predict our target variable (DOC (mg/l)) with respect to the regressors. \n",
    "\n",
    "We are going to use the [statsmodels](https://www.statsmodels.org/stable/index.html) package as it provides more insights for linear regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to normalize our exogenous variables to ensure better convergence of OLS, since there is a high difference in the scale of their domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into features and target\n",
    "X = station_df.drop(columns=[\"DOC (mg/l)\"]).copy()\n",
    "y = station_df[\"DOC (mg/l)\"].copy()\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# revert to dataframe\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "\n",
    "model = sm.OLS(y_train, X_train_scaled).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "X_test_scaled = sm.add_constant(X_test_scaled)\n",
    "\n",
    "# make predictions\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the quality of our regression we can analyse some metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual Sum of Squares**\n",
    "\n",
    "$RSS = \\sum_n (\\hat{t}_n-t_n)^2$, it tells us how much of the prediction differs from the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS = np.sum((y_test - y_pred) ** 2)\n",
    "RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Adjusted) Coefficient of determination**\n",
    "\n",
    "\n",
    "\n",
    "$R^2_{\\text{adj}} = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right)$, where $R^2 = 1 - \\frac{RSS}{\\sum_n (\\bar{t}-t_n)^2}$. It is a modified version of $R^2$ that accounts for the number of predictors (independent variables) in the model. It penalizes the inclusion of unnecessary variables that do not improve the model significantly.\n",
    "\n",
    "$R^2 = 1 - \\frac{RSS}{\\sum_n (\\bar{t}-t_n)^2}$, it tells us how the fraction of the variance of the data explained by the model (how much better we are doing w.r.t. just using the mean of the target $\\bar{t} = \\frac{\\sum_n t_n}{N}$).\n",
    "\n",
    "In spaces with a single feature this is equal to the correlation coefficient between the input and the output;\n",
    "\n",
    "For a more detailed explanation: https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "\n",
    "Statsmodels offers a summary with a lot of information, including $R^2$ and $R^2_{\\text{adj}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Squared Error**\n",
    "\n",
    "$MSE = \\frac{\\sum_n (\\hat{t}_n-t_n)^2}{N}$, it tells approximately how much error we get on a predicted data over the training set (i.e., a normalized version of the RSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the standard linear regression doesn't perform quite well since it makes the strong assumption of linear dependencies between the target variable and the features, which in real-world problems it is usually not the case.\n",
    "\n",
    "More complex models able to capture non-linear relationships are often used. However, understanding which model is best suited for a specific problem is not straightforward. The process of model selection involves different tasks, from feature selection to hyperparameter tuning. Here a first naive comparison between different models is performed.\n",
    "\n",
    "The models we are going to compare are:\n",
    "- Linear Regression\n",
    "- Random Forest\n",
    "- Multi-layer Perceptron Neural Network\n",
    "- XGBoost\n",
    "\n",
    "Since Random Forest, MLP and XGBoost are non-deterministic types of models, to evaluate them we are going to use cross-validation to reduce the impact of randomness of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the linear regression results\n",
    "y_pred_lr = y_pred.copy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into features and target\n",
    "X = station_df.drop(columns=[\"DOC (mg/l)\"]).copy()\n",
    "y = station_df[\"DOC (mg/l)\"].copy()\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# revert to dataframe\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform cross validation for the different models\n",
    "\n",
    "# Random Forest\n",
    "rf_cv = cross_validate(\n",
    "    RandomForestRegressor(),\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5, # 5-fold cross validation\n",
    "    scoring=\"neg_mean_squared_error\", # we want to minimize the mean squared error\n",
    "    return_estimator=True, # to get the fitted models for final evaluation on the test set\n",
    ")\n",
    "\n",
    "# Neural Network\n",
    "nn_cv = cross_validate(\n",
    "    MLPRegressor(max_iter=1000),\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5, # 5-fold cross validation\n",
    "    scoring=\"neg_mean_squared_error\", # we want to minimize the mean squared error\n",
    "    return_estimator=True, # to get the fitted models for final evaluation on the test set\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "xgb_cv = cross_validate(\n",
    "    XGBRegressor(),\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5, # 5-fold cross validation\n",
    "    scoring=\"neg_mean_squared_error\", # we want to minimize the mean squared error\n",
    "    return_estimator=True, # to get the fitted models for final evaluation on the test set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "\n",
    "# Random Forest\n",
    "rf_estimators = rf_cv[\"estimator\"]\n",
    "rf_predictions = np.zeros((X_test.shape[0], len(rf_estimators)))\n",
    "\n",
    "for i, rf in enumerate(rf_estimators):\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "    rf_predictions[:, i] = rf.predict(X_test_scaled)\n",
    "    \n",
    "y_pred_rf = np.mean(rf_predictions, axis=1)\n",
    "\n",
    "# Neural Network\n",
    "nn_estimators = nn_cv[\"estimator\"]\n",
    "nn_predictions = np.zeros((X_test.shape[0], len(nn_estimators)))\n",
    "\n",
    "for i, nn in enumerate(nn_estimators):\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "    nn_predictions[:, i] = nn.predict(X_test_scaled)\n",
    "    \n",
    "y_pred_nn = np.mean(nn_predictions, axis=1)\n",
    "\n",
    "# XGBoost\n",
    "xgb_estimators = xgb_cv[\"estimator\"]\n",
    "xgb_predictions = np.zeros((X_test.shape[0], len(xgb_estimators)))\n",
    "\n",
    "for i, xgb in enumerate(xgb_estimators):\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "    xgb_predictions[:, i] = xgb.predict(X_test_scaled)\n",
    "    \n",
    "y_pred_xgb = np.mean(xgb_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the results\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "print(f\"Linear Regression MSE: {mean_squared_error(y_test, y_pred_lr)}\")\n",
    "\n",
    "# Random Forest\n",
    "print(f\"Random Forest MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "\n",
    "# Neural Network\n",
    "print(f\"Neural Network MSE: {mean_squared_error(y_test, y_pred_nn)}\")\n",
    "\n",
    "# XGBoost\n",
    "print(f\"XGBoost MSE: {mean_squared_error(y_test, y_pred_xgb)}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot Data\n",
    "ax.plot(y_test.index, y_test, color=\"blue\", label=\"Actual\")\n",
    "ax.plot(y_test.index, y_pred_lr, color=\"red\", label=\"Linear Regression\")\n",
    "ax.plot(y_test.index, y_pred_rf, color=\"green\", label=\"Random Forest\")\n",
    "ax.plot(y_test.index, y_pred_nn, color=\"purple\", label=\"Neural Network\")\n",
    "ax.plot(y_test.index, y_pred_xgb, color=\"orange\", label=\"XGBoost\")\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"DOC (mg/l)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: complex models slightly outperform linear regression, but even better results can be achieved with hyperparameter tuning since these models have a lot of hyperparameters that can influence their performance. Also, we can perform different feature selection methods to avoid data-related problems such as multicollinearity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
