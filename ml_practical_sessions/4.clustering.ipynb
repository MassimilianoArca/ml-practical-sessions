{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to see an example of Clustering (K-means) and dimensionality reduction (PCA) on a water quality dataset from MM's Case dell'acqua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data path\n",
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "grab_df = pd.read_excel(os.path.join(data_path, 'grab.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset has two columns related to each parameter, one for the measurement \n",
    "# and one for the label related to the measurement, which can be:\n",
    "# - \"Normal\": the measurement has a valid value\n",
    "# - \"Less than\": the measurement is less than the LOQ\n",
    "# - \"NaN\": the measurement is missing\n",
    "\n",
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the label columns have the name in italian, we will rename them in english\n",
    "feature_mapping = {\n",
    "    \"Cloro residuo libero (al prelievo) (mg/L di Cl2)\": \"Free Chlorine (mg/L)\",\n",
    "    \"Colore (Cu)\": \"Color (CU)\",\n",
    "    \"Concentrazione ioni idrogeno (unità pH)\": \"pH\",\n",
    "    \"Conduttività a 20°C (µS/cm)\": \"Conductivity (uS/cm)\",\n",
    "    \"TOC - carbonio organico totale (mg/L di C)\": \"TOC (mg/L)\",\n",
    "    \"Temperatura (al prelievo) (°C)\": \"Temperature (°C)\",\n",
    "    \"Nitrati (mg/L)\": \"Nitrate (mg/L)\",\n",
    "}\n",
    "\n",
    "targets_mapping = {\n",
    "    \"Batteri coliformi a 37°C (MPN/100 mL)\": \"Coliforms (MPN/100mL)\",\n",
    "    \"Bromodiclorometano (µg/L)\": \"Bromodichloromethane (µg/L)\",\n",
    "    \"Bromoformio (µg/L)\": \"Bromoform (µg/L)\",\n",
    "    \"Cloroformio (µg/L)\": \"Chloroform (µg/L)\",\n",
    "    \"Conta delle colonie a 22°C (UFC/mL)\": \"Colony count at 22°C (UFC/mL)\",\n",
    "    \"Conteggio colonie a 30°C (UFC/mL)\": \"Colony count at 30°C (UFC/mL)\",\n",
    "    \"Conta delle colonie a 37°C (UFC/mL)\": \"Colony count at 37°C (UFC/mL)\",\n",
    "    \"Dibromoclorometano (µg/L)\": \"Dibromochloromethane (µg/L)\",\n",
    "    \"Enterococchi (MPN/100 mL)\": \"Enterococci (MPN/100mL)\",\n",
    "    \"Escherichia coli (MPN/100 mL)\": \"Escherichia coli (MPN/100mL)\",\n",
    "    \"Pseudomonas aeruginosa (UFC/250 mL)\": \"Pseudomonas aeruginosa (UFC/250mL)\",\n",
    "    \"Acido Perfluoroottanoico PFOA (µg/L)\": \"Perfluorooctanoic acid PFOA (µg/L)\",\n",
    "    \"Acido Perfluoroottansolfonico PFOS (µg/L)\": \"Perfluorooctanesulfonic acid PFOS (µg/L)\",\n",
    "    \"Somma di PFAS (µg/L)\": \"Sum of PFAS (µg/L)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import contains\n",
    "\n",
    "# rename the label columns\n",
    "for column in grab_df.columns:\n",
    "    if contains(column, \"label\"):\n",
    "        variable_name = column.split(\"_\")[0]\n",
    "\n",
    "        if variable_name in feature_mapping:\n",
    "            new_name = feature_mapping[variable_name]\n",
    "            new_name = new_name + \"_\" + column.split(\"_\")[1]\n",
    "            grab_df.rename(columns={column: new_name}, inplace=True)\n",
    "\n",
    "        if variable_name in targets_mapping:\n",
    "            new_name = targets_mapping[variable_name]\n",
    "            new_name = new_name + \"_\" + column.split(\"_\")[1]\n",
    "            grab_df.rename(columns={column: new_name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target variables that we are going to use are THMs\n",
    "thms_columns = [\n",
    "    'Bromodichloromethane (µg/L)',\n",
    "    'Bromoform (µg/L)',\n",
    "    'Chloroform (µg/L)',\n",
    "    'Dibromochloromethane (µg/L)'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to store the summary of the data\n",
    "info_df  = pd.DataFrame(\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [\n",
    "            list(feature_mapping.values()) + list(targets_mapping.values()),\n",
    "            [\n",
    "                \"N° Entries\",\n",
    "                \"N° Valid Samples\",\n",
    "                \"% Missing\",\n",
    "                \"N° < LOQ\",\n",
    "                \"Mean Valid\",\n",
    "                \"Std Valid\",\n",
    "                \"LOQ values\",\n",
    "                \"Start Date\",\n",
    "                \"End Date\",\n",
    "            ],\n",
    "        ]\n",
    "    ),\n",
    "    index=grab_df[\"Code\"].unique(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the summary of the data\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in list(feature_mapping.values()) + list(targets_mapping.values()):\n",
    "        df = grab_df[grab_df[\"Code\"] == code][\n",
    "            [\"DateTime\", feature, feature + \"_label\"]\n",
    "        ].copy()\n",
    "\n",
    "        if df.dropna().shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "\n",
    "        start_date = df.dropna()[\"DateTime\"].min().strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.dropna()[\"DateTime\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        df = df[(df[\"DateTime\"] >= start_date) & (df[\"DateTime\"] <= end_date)]\n",
    "\n",
    "        missing_values = (\n",
    "            df[df[feature + \"_label\"].isna()].shape[0] / df.shape[0] * 100\n",
    "        )\n",
    "\n",
    "        info_df.loc[code, (feature, \"N° Entries\")] = df.shape[0]\n",
    "\n",
    "        info_df.loc[code, (feature, \"% Missing\")] = round(missing_values, 2)\n",
    "\n",
    "        info_df.loc[code, (feature, \"N° < LOQ\")] = df[\n",
    "            df[feature + \"_label\"] == \"Less than\"\n",
    "        ].shape[0]\n",
    "        \n",
    "        \n",
    "        valid_df = df[df[feature + \"_label\"] == \"Normal\"]\n",
    "        loq_df = df[df[feature + \"_label\"] == \"Less than\"]\n",
    "        \n",
    "        info_df.loc[code, (feature, \"N° Valid Samples\")] = valid_df.shape[0]\n",
    "        info_df.loc[code, (feature, \"N° < LOQ\")] = loq_df.shape[0]\n",
    "        \n",
    "\n",
    "        info_df.loc[code, (feature, \"Mean Valid\")] = round(valid_df[feature].mean(), 2)\n",
    "        info_df.loc[code, (feature, \"Std Valid\")] = round(valid_df[feature].std(), 2)\n",
    "        \n",
    "        loq_values = loq_df[feature].unique() * 2\n",
    "        loq_values = [str(value) for value in loq_values]\n",
    "        info_df.loc[code, (feature, \"LOQ values\")] = \", \".join(loq_values)\n",
    "\n",
    "        info_df.loc[code, (feature, \"Start Date\")] = start_date\n",
    "        info_df.loc[code, (feature, \"End Date\")] = end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the first level of the columns and maintain the order of the second level\n",
    "info_df = info_df.sort_index(axis=1, level=0, sort_remaining=False, key=lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(info_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation\n",
    "\n",
    "We can see that there are parameters that have a lot of missing values, and some of them have a lot of <LOQ values.\n",
    "\n",
    "We are going to perform two types of imputation:\n",
    "- for the parameters that have measurements below the LOQ, we will impute the missing values with the LOQ/2\n",
    "- for the parameters that have NaN values, we will impute them through [MICE](https://miceforest.readthedocs.io/en/latest/) (Multiple Imputation by Chained Equations). \n",
    "\n",
    "MICE ‘fills in’ (imputes) missing data in a dataset through an iterative series of predictive models. In each iteration, each specified variable in the dataset is imputed using the other variables in the dataset. These iterations should be run until it appears that convergence has been met.\n",
    "\n",
    "Keep in mind that we can only impute the input variables, not the target ones because we would introduce bias and misinformation in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to replace the LOQ values with the LOQ/2\n",
    "def replace_loq(row, column):\n",
    "    return row[column] if row[column + \"_label\"] != \"Less than\" else row[column] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns that contain 'label'\n",
    "label_columns = [col for col in grab_df.columns if 'label' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in grab_df.columns.difference([\"Code\", \"DateTime\"] + label_columns):\n",
    "    grab_df[column] = grab_df.apply(\n",
    "        lambda row: replace_loq(row, column), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the label columns\n",
    "grab_df = grab_df.drop(columns=label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the columns we are interested in\n",
    "grab_df = grab_df[[\"DateTime\", \"Code\"] + list(feature_mapping.values()) + thms_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we set the LOQ values to the valid measurements, we can impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = grab_df[list(feature_mapping.values()) + ['DateTime'] + ['Code']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are rows with all NaN\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    if input_df[input_df['Code'] == code].isnull().all(axis=1).sum() > 0: # Rows with all NaN\n",
    "        print(f'{code} has {input_df[input_df[\"Code\"] == code].isnull().all(axis=1).sum()} rows with all NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No rows with all NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are columns with all NaN\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    if input_df[input_df['Code'] == code].isnull().all(axis=0).sum() > 0: # Columns with all NaN\n",
    "        print(f'{code} has {input_df[input_df[\"Code\"] == code].isnull().all(axis=0).sum()} columns with all NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No columns with all NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to impute the missing values, we need to convert the datetime column to a float\n",
    "# and the code column to a category\n",
    "input_df['DateTime'] = pd.to_numeric(input_df['DateTime'])\n",
    "input_df['Code'] = input_df['Code'].astype('category')\n",
    "\n",
    "input_df = input_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miceforest as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a kernel dataset, able to perform MICE on itself\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=input_df,\n",
    "    variable_schema=input_df.columns.difference(['DateTime', 'Code']).to_list(),\n",
    "    random_state=42,\n",
    "    mean_match_strategy='shap',\n",
    ")\n",
    "\n",
    "kernel.mice(5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the completed dataset\n",
    "completed_dataset = kernel.complete_data(dataset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the distribution of the imputed values is similar to the distribution of the true values\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde, entropy\n",
    "\n",
    "for feature in feature_mapping.values():\n",
    "    sns.histplot(completed_dataset[feature], label='Imputed', kde=True, stat='density')\n",
    "    sns.histplot(grab_df[feature], label='True', kde=True, stat='density')\n",
    "    \n",
    "    # compare the similarity of the distributions, keeping in mind that the non-imputed values are nan \n",
    "    # so we need to remove them from the comparison\n",
    "    # First fit the distributions\n",
    "    kde_completed = gaussian_kde(completed_dataset[feature])\n",
    "    kde_grab = gaussian_kde(grab_df[feature].dropna())\n",
    "    \n",
    "    # create a grid of points to evaluate the distributions\n",
    "    x_vals = np.linspace(min(completed_dataset[feature].min(), grab_df[feature].dropna().min()), max(completed_dataset[feature].max(), grab_df[feature].dropna().max()), 1000)\n",
    "    kde_completed_vals = kde_completed.pdf(x_vals)\n",
    "    kde_grab_vals = kde_grab.pdf(x_vals)\n",
    "    \n",
    "    # normalize the pdfs\n",
    "    kde_completed_vals = kde_completed_vals / kde_completed_vals.sum()\n",
    "    kde_grab_vals = kde_grab_vals / kde_grab_vals.sum()\n",
    "    \n",
    "    # compute the kl divergence between the imputed and the true values\n",
    "    kl_div_value = entropy(kde_completed_vals, kde_grab_vals)\n",
    "    \n",
    "    plt.title(f'{feature} - KL Divergence: {kl_div_value:.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the distributions look similar. As a rule of thumb, if the KL divergence is less than 0.5, the distributions are considered similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the thms columns\n",
    "completed_dataset[thms_columns] = grab_df[thms_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = completed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunatly, there are missing values in the thms columns, so we need to drop them\n",
    "grab_df.dropna(subset=thms_columns, inplace=True)\n",
    "grab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a small amount of data, so it would be useful to cluster data points coming from different drinking water supply points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "We are going to use two clustering algorithms to cluster the data points coming from different supply points.\n",
    "\n",
    "The first version is the hierarchical clustering, where we use the gower distance to cluster the data points.\n",
    "\n",
    "The second version is constrained K-means, where we add the constraint that the data points coming from the same supply points must be in the same cluster.\n",
    "\n",
    "Since K-means requires the number of clusters to be specified, we will use the different metrics to find the optimal number of clusters.\n",
    "- intra-cluster variance weighted by the number of points in the cluster (the lower the better);\n",
    "- silhouette score: how similar an object is to its own cluster (cohesion) compared to other clusters (separation) (the higher the better);\n",
    "- davies-bouldin index: measures the average similarity between each cluster and the most similar cluster (the lower the better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from gower import gower_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the datetime column as it is not a feature and could wrongly bias the clustering\n",
    "df = grab_df[grab_df.columns.difference(['DateTime'])]\n",
    "df[\"Code\"] = df[\"Code\"].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = gower_matrix(df)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(distance_matrix, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "dendrogram(linkage_matrix, labels=df.index, leaf_rotation=90)\n",
    "\n",
    "plt.title(\"Hierarchical Clustering with Gower's Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the optimal number of clusters by looking at the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "scores = []\n",
    "\n",
    "for k in range(2, 10): \n",
    "    clusters = fcluster(linkage_matrix, k, criterion='maxclust')\n",
    "    score = silhouette_score(distance_matrix, clusters, metric='precomputed')\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 10), scores, label='Silhouette Score')\n",
    "\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Scores')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the clusters are coherent with the DW supply points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_indexes = [4, 6, 18, 42]\n",
    "cluster1_indexes = grab_df.index.difference(cluster0_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_df = grab_df.iloc[cluster0_indexes]\n",
    "cluster1_df = grab_df.iloc[cluster1_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some points coming from the same DW supply point in both clusters, which is not what we want.\n",
    "\n",
    "So we need to find a way to cluster the data points coming from the same DW supply point in the same cluster.\n",
    "\n",
    "One way to do this is to use Constraint K-means, where we add the constraint that the data points coming from the same DW supply point must be in the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to drop also the code column as k-means is not able to handle categorical variables\n",
    "df = grab_df[grab_df.columns.difference(['DateTime', 'Code'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "variances = []\n",
    "sil_scores = []\n",
    "db_scores = []\n",
    "\n",
    "# define the number of points\n",
    "total_points = df.shape[0]\n",
    "\n",
    "# compute the variance also for the case of 1 cluster\n",
    "variance = np.var(df, axis=0).mean()\n",
    "\n",
    "variances.append(variance)\n",
    "\n",
    "for n_cluster in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=n_cluster, random_state=12)\n",
    "    clusters = kmeans.fit_predict(df)\n",
    "    \n",
    "    sil_score = silhouette_score(df, clusters)\n",
    "    db_score = davies_bouldin_score(df, clusters)\n",
    "\n",
    "    df['Cluster'] = clusters\n",
    "    \n",
    "    # compute the variance of each cluster\n",
    "    variance = 0\n",
    "    for cluster in df['Cluster'].unique():\n",
    "        cluster_df = df[df['Cluster'] == cluster].copy()\n",
    "        variance += np.var(cluster_df, axis=0).mean() * cluster_df.shape[0] / total_points\n",
    "    \n",
    "    # compute average variance for n_cluster\n",
    "    variance /= n_cluster    \n",
    "    \n",
    "    variances.append(variance)\n",
    "    sil_scores.append(sil_score)\n",
    "    db_scores.append(db_score)\n",
    "    \n",
    "    df.drop(columns=['Cluster'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the scores\n",
    "variances = np.array(variances)\n",
    "sil_scores = np.array(sil_scores)\n",
    "db_scores = np.array(db_scores)\n",
    "\n",
    "variances = scaler.fit_transform(variances.reshape(-1, 1)).flatten()\n",
    "sil_scores = scaler.fit_transform(sil_scores.reshape(-1, 1)).flatten()\n",
    "db_scores = scaler.fit_transform(db_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "plt.plot(range(1, 10), variances, label='Weighted Average Variance')\n",
    "plt.plot(range(2, 10), sil_scores, label='Silhouette Score')\n",
    "plt.plot(range(2, 10), db_scores, label='Davies Bouldin Score')\n",
    "\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('(Normalized) Scores')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of clusters is 2. Evaluate the clustering with similarity matrix inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import heatmap\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(df)\n",
    "\n",
    "clusters = kmeans.predict(df)\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "distance_matrix = euclidean_distances(df)\n",
    "similarity_matrix = 1 / (1 + distance_matrix)\n",
    "\n",
    "sorted_indices = np.argsort(df['Cluster'])\n",
    "sorted_matrix = similarity_matrix[sorted_indices, :][:, sorted_indices]\n",
    "\n",
    "heatmap(sorted_matrix, cmap='viridis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = euclidean_distances(df)\n",
    "similarity_matrix = 1 / (1 + distance_matrix)\n",
    "\n",
    "sorted_indices = np.random.permutation(df.index)\n",
    "sorted_matrix = similarity_matrix[sorted_indices, :][:, sorted_indices]\n",
    "\n",
    "heatmap(sorted_matrix, cmap='viridis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COP K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copkmeans.cop_kmeans import cop_kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataframe\n",
    "df = grab_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the feature + thms columns\n",
    "joint_columns = list(feature_mapping.values()) + thms_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "df[joint_columns] = pd.DataFrame(scaler.fit_transform(df[joint_columns]), columns=joint_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# define a list of constraints, where every element is a list of the form [index1, index2]\n",
    "# that means that the data points with the indexes index1 and index2 must be in the same cluster\n",
    "must_link = []\n",
    "\n",
    "\n",
    "for code in df['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(df[df['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "# convert the dataframe to a numpy array to pass it to the cop_kmeans function\n",
    "np_df = df[joint_columns].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "variances = []\n",
    "sil_scores = []\n",
    "db_scores = []\n",
    "\n",
    "# compute the variance also for the case of 1 cluster\n",
    "variance = np.var(np_df, axis=0).mean()\n",
    "\n",
    "variances.append(variance)\n",
    "\n",
    "# define the number of points\n",
    "total_points = df.shape[0]\n",
    "\n",
    "for n_cluster in range(2, 10):\n",
    "    clusters, centers = cop_kmeans(np_df, n_cluster, ml=must_link)\n",
    "    \n",
    "    sil_score = silhouette_score(np_df, clusters)\n",
    "    db_score = davies_bouldin_score(np_df, clusters)\n",
    "\n",
    "    df['Cluster'] = clusters\n",
    "    \n",
    "    # compute the variance of each cluster\n",
    "    variance = 0\n",
    "    for cluster in df['Cluster'].unique():\n",
    "        cluster_df = df[df['Cluster'] == cluster].copy()\n",
    "        variance += np.var(cluster_df[joint_columns], axis=0).mean() * cluster_df.shape[0] / total_points\n",
    "    \n",
    "    # compute average variance for n_cluster\n",
    "    variance /= n_cluster    \n",
    "    \n",
    "    variances.append(variance)\n",
    "    sil_scores.append(sil_score)\n",
    "    db_scores.append(db_score)\n",
    "    \n",
    "    df.drop(columns=['Cluster'], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the scores\n",
    "variances = np.array(variances)\n",
    "sil_scores = np.array(sil_scores)\n",
    "db_scores = np.array(db_scores)\n",
    "\n",
    "variances = scaler.fit_transform(variances.reshape(-1, 1)).flatten()\n",
    "sil_scores = scaler.fit_transform(sil_scores.reshape(-1, 1)).flatten()\n",
    "db_scores = scaler.fit_transform(db_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "plt.plot(range(1, 10), variances, label='Weighted Average Variance')\n",
    "plt.plot(range(2, 10), sil_scores, label='Silhouette Score')\n",
    "plt.plot(range(2, 10), db_scores, label='Davies Bouldin Score')\n",
    "\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('(Normalized) Scores')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score is the highest for 3 clusters, the davies-bouldin score is the lowest for 3 clusters. The weighted average variance would suggest 4 clusters, but all together the best number of clusters is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "\n",
    "must_link = []\n",
    "\n",
    "\n",
    "for code in df['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(df[df['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = df[joint_columns].to_numpy()\n",
    "\n",
    "clusters, centers = cop_kmeans(np_df, n_clusters, ml=must_link,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in df['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = df[df['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    \n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = euclidean_distances(df[joint_columns])\n",
    "similarity_matrix = 1 / (1 + distance_matrix)\n",
    "\n",
    "sorted_indices = np.argsort(df['Cluster'])\n",
    "sorted_matrix = similarity_matrix[sorted_indices, :][:, sorted_indices]\n",
    "\n",
    "heatmap(sorted_matrix, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a better visualization, we can try to perform clustering on a subset of the variables in the dataset.\n",
    "\n",
    "However, in order to lose much less information, we can perform PCA on the dataset and then perform clustering on the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COP K-means on Temperature, Free Chlorine and TTHMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to perform clustering on the temperature, free chlorine and TTHMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df['TTHMs'] = grab_df[thms_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_columns = ['Temperature (°C)', 'Free Chlorine (mg/L)', 'TTHMs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grab_df[subset_columns + ['Code']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "df[subset_columns] = pd.DataFrame(scaler.fit_transform(df[subset_columns]), columns=subset_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the must link\n",
    "must_link = []\n",
    "\n",
    "for code in df['Code'].unique():\n",
    "    index_pairs = list(combinations(df[df['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = df[subset_columns].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = []\n",
    "sil_scores = []\n",
    "db_scores = []\n",
    "\n",
    "# compute the variance also for the case of 1 cluster\n",
    "variance = np.var(np_df, axis=0).mean()\n",
    "\n",
    "variances.append(variance)\n",
    "\n",
    "# define the number of points\n",
    "total_points = df.shape[0]\n",
    "\n",
    "for n_cluster in range(2, 10):\n",
    "    clusters, centers = cop_kmeans(np_df, n_cluster, ml=must_link)\n",
    "    \n",
    "    sil_score = silhouette_score(np_df, clusters)\n",
    "    db_score = davies_bouldin_score(np_df, clusters)\n",
    "\n",
    "    df['Cluster'] = clusters\n",
    "    \n",
    "    # compute the variance of each cluster\n",
    "    variance = 0\n",
    "    for cluster in df['Cluster'].unique():\n",
    "        cluster_df = df[df['Cluster'] == cluster].copy()\n",
    "        variance += np.var(cluster_df[subset_columns], axis=0).mean() * cluster_df.shape[0] / total_points\n",
    "    \n",
    "    # compute average variance for n_cluster\n",
    "    variance /= n_cluster    \n",
    "    \n",
    "    variances.append(variance)\n",
    "    sil_scores.append(sil_score)\n",
    "    db_scores.append(db_score)\n",
    "    \n",
    "    df.drop(columns=['Cluster'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the scores\n",
    "variances = np.array(variances)\n",
    "sil_scores = np.array(sil_scores)\n",
    "db_scores = np.array(db_scores)\n",
    "\n",
    "variances = scaler.fit_transform(variances.reshape(-1, 1)).flatten()\n",
    "sil_scores = scaler.fit_transform(sil_scores.reshape(-1, 1)).flatten()\n",
    "db_scores = scaler.fit_transform(db_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "plt.plot(range(1, 10), variances, label='Weighted Average Variance')\n",
    "plt.plot(range(2, 10), sil_scores, label='Silhouette Score')\n",
    "plt.plot(range(2, 10), db_scores, label='Davies Bouldin Score')\n",
    "\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('(Normalized) Scores')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select 2 as the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "\n",
    "must_link = []\n",
    "\n",
    "\n",
    "for code in df['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(df[df['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = df[subset_columns].to_numpy()\n",
    "\n",
    "clusters, centers = cop_kmeans(np_df, n_clusters, ml=must_link,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in df['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = df[df['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    \n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the clusters in a 3D scatter plot with plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Scatter3d(\n",
    "        x=df[df['Cluster'] == cluster]['Temperature (°C)'],\n",
    "        y=df[df['Cluster'] == cluster]['Free Chlorine (mg/L)'],\n",
    "        z=df[df['Cluster'] == cluster]['TTHMs'],\n",
    "        mode='markers',\n",
    "        name=f'Cluster {cluster}',\n",
    "        marker=dict(size=5), \n",
    "    ) for cluster in df['Cluster'].unique()\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Temperature (°C)',\n",
    "        yaxis_title='Free Chlorine (mg/L)', \n",
    "        zaxis_title='TTHMs'\n",
    "    ),\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show(\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the boxplot of the TTHMs for each cluster\n",
    "import plotly.express as px\n",
    "fig = px.box(df, x='Cluster', y='TTHMs', points='all')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Principal Component Analysis is a technique that allows us to reduce the dimensionality of the dataset by projecting the data onto a lower-dimensional space. It can also be used as a model selection technique to select the most important features.\n",
    "\n",
    "We are to use PCA in a supervised way, by trying to predict the TTHMs from the other variables.\n",
    "\n",
    "We are going to compare three different approaches:\n",
    "- Linear Regression with all the variables\n",
    "- Linear Regression with 2 input variables based on expert knowledge (temperature and free chlorine)\n",
    "- Linear Regression with the principal components\n",
    "\n",
    "We are going to use the principal components as features for the Linear Regression.\n",
    "\n",
    "Comparison is done with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the variables\n",
    "df = grab_df[list(feature_mapping.values()) + thms_columns].copy()\n",
    "df['TTHMs'] = df[thms_columns].sum(axis=1)\n",
    "df = df[list(feature_mapping.values()) + ['TTHMs']]\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "df[list(feature_mapping.values())] = pd.DataFrame(scaler.fit_transform(df[list(feature_mapping.values())]), columns=list(feature_mapping.values()))\n",
    "\n",
    "# perform Linear Regression with all the variables with cross-validation\n",
    "lr = LinearRegression()\n",
    "scores = cross_val_score(lr, df[list(feature_mapping.values())], df['TTHMs'], cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Validation MSE: \", -scores.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Linear Regression with 2 input variables based on expert knowledge (temperature and free chlorine)\n",
    "lr = LinearRegression()\n",
    "scores = cross_val_score(lr, df[['Temperature (°C)', 'Free Chlorine (mg/L)']], df['TTHMs'], cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Validation MSE: \", -scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first plot the cumulative explained variance ratio\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = grab_df[list(feature_mapping.values()) + thms_columns].copy()\n",
    "df['TTHMs'] = df[thms_columns].sum(axis=1)\n",
    "df = df[list(feature_mapping.values()) + ['TTHMs']]\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "df[list(feature_mapping.values())] = pd.DataFrame(scaler.fit_transform(df[list(feature_mapping.values())]), columns=list(feature_mapping.values()))\n",
    "\n",
    "pca = PCA(n_components=len(list(feature_mapping.values())))\n",
    "pca.fit(df[list(feature_mapping.values())])\n",
    "\n",
    "plt.plot(range(1, len(list(feature_mapping.values())) + 1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we take 5 components, we explain more than 90% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "\n",
    "df = grab_df[list(feature_mapping.values()) + thms_columns].copy()\n",
    "df['TTHMs'] = df[thms_columns].sum(axis=1)\n",
    "df = df[list(feature_mapping.values()) + ['TTHMs']]\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "df[list(feature_mapping.values())] = pd.DataFrame(scaler.fit_transform(df[list(feature_mapping.values())]), columns=list(feature_mapping.values()))\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(df[list(feature_mapping.values())])\n",
    "\n",
    "df_pca = pca.fit_transform(df[list(feature_mapping.values())])\n",
    "df_pca = pd.DataFrame(df_pca, columns=[f'PC{i}' for i in range(1, n_components + 1)])\n",
    "df = pd.concat([df_pca, df['TTHMs']], axis=1)\n",
    "\n",
    "\n",
    "# perform Linear Regression with the principal components with cross-validation\n",
    "lr = LinearRegression()\n",
    "scores = cross_val_score(lr, df[[f'PC{i}' for i in range(1, n_components + 1)]], df['TTHMs'], cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Validation MSE: \", -scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
